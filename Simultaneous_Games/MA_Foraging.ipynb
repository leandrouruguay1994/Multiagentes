{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89a56b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lbforaging\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "os.environ['WANDB_SILENT'] = \"false\"\n",
    "os.environ['WANDB_START_METHOD'] = \"thread\"\n",
    "os.environ['WANDB_MODE'] = \"online\"\n",
    "\n",
    "from games.foraging import Foraging\n",
    "from agents.iql_agent import IQLAgent, IQLAgentConfig\n",
    "from agents.jal_am_agent import JALAgent, JALAgentConfig\n",
    "from utils import plot_training_results\n",
    "import wandb\n",
    "from nbconvert import HTMLExporter\n",
    "import nbformat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d195856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_login():\n",
    "    wandb.login()\n",
    "\n",
    "def setup_wandb(config, project_name=\"foraging_rl\", group=\"jal_vs_iql\"):\n",
    "    \"\"\"Configura Weights & Biases para tracking de experimentos\"\"\"\n",
    "    if wandb.run is not None:\n",
    "        wandb.finish()\n",
    "\n",
    "    # Generar nombre descriptivo para la corrida\n",
    "    env_name = config['game'].split('-')[0]\n",
    "    grid_size = config['game'].split('-')[1]\n",
    "    players = config['game'].split('-')[2]\n",
    "    food = config['game'].split('-')[3]\n",
    "    version = config['game'].split('-')[4]\n",
    "    \n",
    "    # Contar tipos de agentes\n",
    "    agent_counts = {'jal': 0, 'iql': 0}\n",
    "    for agent_type in config['agent_types'].values():\n",
    "        agent_counts[agent_type] += 1\n",
    "    \n",
    "    run_name = (f\"{env_name}_{grid_size}_{players}p_{food}f_\"\n",
    "               f\"JAL{agent_counts['jal']}-IQL{agent_counts['iql']}_\"\n",
    "               f\"{time.strftime('%m%d-%H%M')}\")\n",
    "    \n",
    "    # Estructurar mejor la configuración para W&B\n",
    "    wandb_config = {\n",
    "        'environment': {\n",
    "            'name': env_name,\n",
    "            'grid_size': grid_size,\n",
    "            'players': players,\n",
    "            'food': food,\n",
    "            'version': version,\n",
    "            'cooperative': 'coop' in config['game']\n",
    "        },\n",
    "        'training': {\n",
    "            'episodes': config['train_config']['episodes'],\n",
    "            'iterations': config['train_config']['iterations'],\n",
    "            'total_episodes': config['train_config']['episodes'] * config['train_config']['iterations']\n",
    "        },\n",
    "        'agents': {\n",
    "            'distribution': agent_counts,\n",
    "            'types': config['agent_types']\n",
    "        },\n",
    "        'hyperparameters': {}\n",
    "    }\n",
    "\n",
    "    # Agregar hiperparámetros específicos de cada tipo de agente\n",
    "    for agent_type in ['jal', 'iql']:\n",
    "        if agent_type in config:\n",
    "            wandb_config['hyperparameters'][agent_type] = vars(config[agent_type])\n",
    "\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=project_name,\n",
    "        entity=\"rizzo33-universidad-ort-uruguay\",  # Cambia esto por tu entidad de W&B\n",
    "        group=group,\n",
    "        name=run_name,\n",
    "        config=wandb_config,\n",
    "        save_code=False,\n",
    "        reinit=True,\n",
    "        settings=wandb.Settings(\n",
    "            disable_job_creation=True,  # Evitar problemas con notebooks\n",
    "            silent=False                # Reducir mensajes de consola\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Guardar metadata adicional como tags\n",
    "    tags = [\n",
    "        f\"{grid_size}\",\n",
    "        f\"{players}p\",\n",
    "        f\"{food}f\",\n",
    "        f\"JAL{agent_counts['jal']}\",\n",
    "        f\"IQL{agent_counts['iql']}\",\n",
    "        \"coop\" if 'coop' in config['game'] else \"comp\"\n",
    "    ]\n",
    "    run.tags = tags\n",
    "    \n",
    "    return run\n",
    "\n",
    "def save_notebook_to_html(notebook_path=\"MA_Foraging.ipynb\"):\n",
    "    \"\"\"Guarda el notebook como HTML\"\"\"\n",
    "    exporter = HTMLExporter()\n",
    "    notebook = nbformat.read(notebook_path, as_version=4)\n",
    "    html, _ = exporter.from_notebook_node(notebook)\n",
    "    \n",
    "    os.makedirs(\"notebook_exports\", exist_ok=True)\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    output_path = f\"notebook_exports/experiment_{timestamp}.html\"\n",
    "    \n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(html)\n",
    "    \n",
    "    if wandb.run:\n",
    "        wandb.save(output_path)\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41f01c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/martinrizzo/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrizzo33\u001b[0m (\u001b[33mrizzo33-universidad-ort-uruguay\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"8c15e4a23489e53756d79a4ac60076b117e45813\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bddb364",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_config = [\n",
    "        \"Foraging-5x5-2p-1f-v3\",\n",
    "        \"Foraging-8x8-2p-1f-v3\",\n",
    "        \"Foraging-8x8-3p-1f-v3\",\n",
    "        \"Foraging-8x8-3p-1f-coop-v3\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78c9acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración mixta\n",
    "mixed_config = {\n",
    "    'game': game_config[0],  # Foraging-5x5-2p-1f-v3\n",
    "    'iql': IQLAgentConfig(alpha=0.1, min_epsilon=0.01, max_t=10000, seed=1),\n",
    "    'jal': JALAgentConfig(alpha=0.1, min_epsilon=0.05, epsilon_decay=0.99999, max_t=10000, seed=1),\n",
    "    'train_config': {\n",
    "        'episodes': 100,\n",
    "        'iterations': 50,\n",
    "    },\n",
    "    'eval_config': {\n",
    "    'episodes': 20,\n",
    "    },\n",
    "    'agent_types': {\n",
    "        'agent_0': 'jal',  # Primer agente usa JAL\n",
    "        'agent_1': 'jal'   # Segundo agente usa IQL\n",
    "    }\n",
    "}\n",
    "\n",
    "# Configuración de IQL vs IQL\n",
    "iql_config = {\n",
    "    'game': game_config[0],  # Foraging-5x5-2p-1f-v3\n",
    "    'iql': IQLAgentConfig(alpha=0.1, min_epsilon=0.01, epsilon_decay=0.99995, max_t=10000, seed=1),\n",
    "    'jal': JALAgentConfig(alpha=0.1, min_epsilon=0.05, epsilon_decay=0.99999, max_t=10000, seed=1),\n",
    "    'train_config': {\n",
    "        'episodes': 100,\n",
    "        'iterations': 50,\n",
    "    },\n",
    "    'eval_config': {\n",
    "    'episodes': 20,\n",
    "    },\n",
    "    'agent_types': {\n",
    "        'agent_0': 'iql',  # Primer agente usa JAL\n",
    "        'agent_1': 'iql'   # Segundo agente usa IQL\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "coop_config = {\n",
    "    'game': game_config[3],  # Foraging-8x8-3p-1f-coop-v3\n",
    "    'iql': IQLAgentConfig(alpha=0.1, min_epsilon=0.01, epsilon_decay=0.99995, max_t=10000, seed=1),\n",
    "    'jal': JALAgentConfig(alpha=0.1, min_epsilon=0.05, epsilon_decay=0.99999, max_t=10000, seed=1),\n",
    "    'train_config': {\n",
    "        'episodes': 200,\n",
    "        'iterations': 50,\n",
    "    },\n",
    "    'eval_config': {\n",
    "        'episodes': 20,\n",
    "    },\n",
    "    'agent_types': {\n",
    "        'agent_0': 'iql',  # Primer agente usa JAL\n",
    "        'agent_1': 'iql',   # Segundo agente usa JAL\n",
    "        'agent_2': 'iql'   # Tercer agente usa JAL\n",
    "    }\n",
    "}\n",
    "#wandb = setup_wandb(mixed_config, project_name=\"mixed_foraging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b463e8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/pettingzoo_games/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "# Crear el juego\n",
    "game = Foraging(config=mixed_config['game'], seed=1)\n",
    "\n",
    "# Crear agentes mixtos\n",
    "agents = {}\n",
    "for agent_id in game.agents:\n",
    "    if mixed_config['agent_types'].get(agent_id, 'iql') == 'jal':\n",
    "        agents[agent_id] = JALAgent(game, agent_id, coop_config['jal'])\n",
    "    else:\n",
    "        agents[agent_id] = IQLAgent(game, agent_id, coop_config['iql'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85dadbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: agent_0\n",
      "Observation: [3. 1. 2. 2. 3. 2. 0. 0. 2.]\n",
      "Agent: agent_1\n",
      "Observation: [3. 1. 2. 0. 0. 2. 2. 3. 2.]\n"
     ]
    }
   ],
   "source": [
    "game.reset()\n",
    "for agent in game.agents:\n",
    "    print(f\"Agent: {agent}\")\n",
    "    print(f\"Observation: {game.observe(agent)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbc0878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(game, agents, verbose=False, render=False):\n",
    "\n",
    "    # Initialize the game\n",
    "    game.reset()\n",
    "    step_count = 0\n",
    "\n",
    "    # Initialize each agent\n",
    "    for agent in game.agents:\n",
    "        agents[agent].reset()\n",
    "\n",
    "    # Print initial observations if verbose is enabled\n",
    "    if verbose:\n",
    "        print(f\"Step: {step_count}\")\n",
    "        for agent in game.agents:\n",
    "            print(f\"Agent {agent} observe: {game.observe(agent)}\")\n",
    "\n",
    "    # Initialize rewards for each agent\n",
    "    cum_rewards = dict(map(lambda agent: (agent, 0.0), game.agents))\n",
    "    # render the game if required\n",
    "    if render:\n",
    "        game.render()\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    while not game.done():\n",
    "\n",
    "        step_count += 1\n",
    "        \n",
    "        # Get actions from each agent\n",
    "        actions = {}\n",
    "        for agent in game.agents:\n",
    "            actions[agent] = agents[agent].action()\n",
    "             \n",
    "        # Perform the actions in the game\n",
    "        game.step(actions)\n",
    "\n",
    "        # Update the cum_rewards for each agent\n",
    "        for agent in game.agents:\n",
    "            reward = game.reward(agent)\n",
    "            cum_rewards[agent] += reward\n",
    "\n",
    "        # Print actions, rewards and next state if verbose is enabled\n",
    "        if verbose:\n",
    "            print(f\"Step: {step_count}\")\n",
    "            for agent in game.agents:\n",
    "                    print(f\"Agent {agent} action: {actions[agent]} - {game.action_set[actions[agent]]}\")\n",
    "                    print(f\"Agent {agent} reward: {game.reward(agent)}\")\n",
    "                    print(f\"Agent {agent} observe: {game.observe(agent)}\")\n",
    "            \n",
    "        if render:\n",
    "            game.render()\n",
    "            time.sleep(0.5)\n",
    "    \n",
    "        for agent in game.agents:\n",
    "            # Update the agent with the last observation\n",
    "            agents[agent].update()\n",
    "    \n",
    "    return cum_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfa8ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(game, agents, episodes=1, verbose=False, render=False, log_wandb=False):\n",
    "    sum_rewards = dict(map(lambda agent: (agent, 0.0), game.agents))\n",
    "    episodes_length = []\n",
    "    for _ in range(episodes):\n",
    "        episode_start = time.time()\n",
    "        cum_rewards = play_episode(game, agents, verbose=verbose, render=render)\n",
    "        episode_time = time.time() - episode_start\n",
    "        for agent in game.agents:\n",
    "            sum_rewards[agent] += cum_rewards[agent]\n",
    "        episodes_length.append(episode_time)\n",
    "\n",
    "        if log_wandb and wandb.run:\n",
    "            log_data = {f\"episode_reward/{agent}\": reward for agent, reward in cum_rewards.items()}\n",
    "            log_data[\"episode_length\"] = episode_time\n",
    "            wandb.log(log_data)\n",
    "        \n",
    "    if verbose:\n",
    "        print(f\"Average rewards over {episodes} episodes:\")\n",
    "        for agent in game.agents:\n",
    "            print(f\"Agent {agent}: {sum_rewards[agent] / episodes}\")\n",
    "    avg_rewards = {agent: sum_rewards[agent] / episodes for agent in game.agents}        \n",
    "    if log_wandb and wandb.run:\n",
    "        wandb.log({f\"avg_reward/{agent}\": avg_r for agent, avg_r in avg_rewards.items()})\n",
    "        wandb.log({\"avg_episode_time\": np.mean(episodes_length)})\n",
    "      \n",
    "    return sum_rewards     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3727d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(game, agents, train_config, progress=False, verbose=False, render=False):\n",
    "    iterations = train_config[\"iterations\"]\n",
    "    episodes = train_config[\"episodes\"]\n",
    "    total_episodes = iterations * episodes\n",
    "    \n",
    "    # Estadísticas de entrenamiento\n",
    "    stats = {\n",
    "        'average_rewards': {agent: [] for agent in game.agents},\n",
    "        'episode_rewards': {agent: [] for agent in game.agents},\n",
    "        'exploration_rates': {agent: [] for agent in game.agents},\n",
    "        'time_per_episode': [],\n",
    "        'time_per_iteration': [],\n",
    "        'total_time': 0,\n",
    "        'start_time': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        'end_time': None,\n",
    "        'agent_types': {agent: type(agents[agent]).__name__ for agent in game.agents},\n",
    "        'environment': str(game.env)\n",
    "    }\n",
    "    optimal_reward_per_episode = 1.0 #para roraging\n",
    "\n",
    "    print(f\"🚀 Starting training for {iterations} iterations ({total_episodes} total episodes)\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Configurar barra de progreso principal\n",
    "    pbar = tqdm(total=total_episodes, desc=\"Training Progress\", unit=\"episode\")#, disable=not progress)\n",
    "    \n",
    "    for i in range(1, iterations + 1):\n",
    "        iteration_start = time.time()\n",
    "        iteration_rewards = {agent: 0.0 for agent in game.agents}\n",
    "        iteration_exploration = {agent: 0.0 for agent in game.agents}\n",
    "        \n",
    "        for e in range(1, episodes + 1):\n",
    "            episode_start = time.time()\n",
    "            episode_num = (i - 1) * episodes + e\n",
    "            \n",
    "            # Ejecutar episodio\n",
    "            cum_rewards = play_episode(game, agents, verbose=verbose, render=render)\n",
    "            \n",
    "            # Calcular tiempo del episodio\n",
    "            episode_time = time.time() - episode_start\n",
    "            stats['time_per_episode'].append(episode_time)\n",
    "\n",
    "            # Actualizar estadísticas\n",
    "            for agent in game.agents:\n",
    "                stats['episode_rewards'][agent].append(cum_rewards[agent])\n",
    "                iteration_rewards[agent] += cum_rewards[agent]\n",
    "\n",
    "\n",
    "                if hasattr(agents[agent], 'epsilon'):\n",
    "                    iteration_exploration[agent] += agents[agent].epsilon\n",
    "                elif hasattr(agents[agent], 'config') and hasattr(agents[agent].config, 'initial_epsilon'):\n",
    "                    iteration_exploration[agent] += agents[agent].config.initial_epsilon\n",
    "            # Loggear en W&B\n",
    "            if wandb.run:\n",
    "                log_data = {\n",
    "                    **{f\"live_rewards/{agent}\": cum_rewards[agent] for agent in game.agents},\n",
    "                    \"episode\": episode_num,\n",
    "                    \"episode_time\": episode_time,\n",
    "                    \"global_step\": episode_num\n",
    "                }\n",
    "\n",
    "                # Agregar exploración si está disponible\n",
    "                for agent in game.agents:\n",
    "                    if hasattr(agents[agent], 'epsilon'):\n",
    "                        log_data[f\"exploration/{agent}\"] = agents[agent].epsilon\n",
    "\n",
    "                wandb.log(log_data)\n",
    "            pbar.update(1)\n",
    "        \n",
    "\n",
    "        \n",
    "        # Tiempo por iteración\n",
    "        iteration_time = time.time() - iteration_start\n",
    "        stats['time_per_iteration'].append(iteration_time)\n",
    "        \n",
    "        # Guardar promedios por iteración\n",
    "        for agent in game.agents:\n",
    "            avg_reward = iteration_rewards[agent] / episodes\n",
    "            stats['average_rewards'][agent].append(avg_reward)\n",
    "            stats['exploration_rates'][agent].append(iteration_exploration[agent] / episodes)\n",
    "\n",
    "            # Loggear en W&B\n",
    "            if wandb.run:\n",
    "                wandb.log({\n",
    "                    f\"iteration_avg_reward/{agent}\": avg_reward,\n",
    "                    f\"exploration_rate/{agent}\": stats['exploration_rates'][agent][-1],\n",
    "                    \"iteration\": i,\n",
    "                    \"iteration_time\": iteration_time,\n",
    "                    \"global_step\": i * episodes\n",
    "                })\n",
    "\n",
    "    # Finalizar medición de tiempo\n",
    "    stats['total_time'] = time.time() - start_time\n",
    "    stats['end_time'] = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    pbar.close()\n",
    "\n",
    "    # Guardar resultados finales en W&B\n",
    "    if wandb.run:\n",
    "        # Métricas resumen\n",
    "        summary_metrics = {\n",
    "            \"total_training_time\": stats['total_time'],\n",
    "            \"avg_episode_time\": np.mean(stats['time_per_episode']),\n",
    "            \"avg_iteration_time\": np.mean(stats['time_per_iteration']),\n",
    "            \"episodes_per_second\": total_episodes / stats['total_time']\n",
    "        }\n",
    "        # Agregar recompensas promedio por agente\n",
    "        for agent in game.agents:\n",
    "            summary_metrics[f\"final_avg_reward/{agent}\"] = np.mean(stats['episode_rewards'][agent][-episodes:])\n",
    "\n",
    "        wandb.summary.update(summary_metrics)\n",
    "        \n",
    "        # Guardar figuras en W&B\n",
    "        figures = plot_training_results(stats, agents, game, save_local=False)\n",
    "        for name, fig in figures.items():\n",
    "            wandb.log({f\"charts/{name}\": wandb.Image(fig)})\n",
    "            plt.close(fig)\n",
    "            \n",
    "        # Guardar configuración de los agentes\n",
    "        for agent_id, agent in agents.items():\n",
    "            if hasattr(agent, 'config'):\n",
    "                wandb.config.update({f\"agent_{agent_id}_config\": vars(agent.config)}, allow_val_change=True)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aedb0060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q_table = agents[game.agents[0]].q_table\n",
    "#print(f\"Q-table for agent {game.agents[0]}:\")\n",
    "#print(f\"Estados: {len(q_table.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae4961c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for agent in game.agents:\n",
    "#    agents[agent].learn = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a719562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run(game, agents, episodes=10, verbose=True, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3342562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_combined_results(train_stats, eval_stats, agents):\n",
    "    \"\"\"Visualización combinada de entrenamiento y evaluación\"\"\"\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    # Gráfico 1: Progreso de entrenamiento\n",
    "    plt.subplot(1, 3, 1)\n",
    "    for agent_id in train_stats['average_rewards']:\n",
    "        plt.plot(train_stats['average_rewards'][agent_id], \n",
    "                label=f\"{agent_id} ({type(agents[agent_id]).__name__})\")\n",
    "    plt.title(\"Training Progress\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Average Reward\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gráfico 2: Recompensas de evaluación\n",
    "    plt.subplot(1, 3, 2)\n",
    "    for agent_id in eval_stats['episode_rewards']:\n",
    "        rewards = eval_stats['episode_rewards'][agent_id]\n",
    "        plt.scatter(range(len(rewards)), rewards, alpha=0.5, \n",
    "                   label=f\"{agent_id} eval rewards\")\n",
    "    plt.title(\"Evaluation Episode Rewards\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gráfico 3: Distribución de acciones en evaluación\n",
    "    plt.subplot(1, 3, 3)\n",
    "    for agent_id in eval_stats['actions_taken']:\n",
    "        actions = eval_stats['actions_taken'][agent_id]\n",
    "        plt.hist(actions, alpha=0.5, bins=20, \n",
    "                label=f\"{agent_id} actions\")\n",
    "    plt.title(\"Evaluation Action Distribution\")\n",
    "    plt.xlabel(\"Action\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar en W&B\n",
    "    if wandb.run:\n",
    "        wandb.log({\"combined_results\": plt})\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e04231c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_configuration(config):\n",
    "    # Inicializar W&B\n",
    "    setup_wandb(config, project_name=\"foraging_experiments\", group='jal vs iql')\n",
    "    game = Foraging(config=config['game'], seed=config.get('seed', 1))\n",
    "    \n",
    "    agents = {}\n",
    "    for agent_id in game.agents:\n",
    "        agent_type = config['agent_types'].get(agent_id, 'iql')\n",
    "        if agent_type == 'jal':\n",
    "            agents[agent_id] = JALAgent(game, agent_id, config['jal'])\n",
    "        else:\n",
    "            agents[agent_id] = IQLAgent(game, agent_id, config['iql'])\n",
    "    \n",
    "    rewards = train(game, agents, config['train_config'], progress=True)\n",
    "    \n",
    "    # Visualización\n",
    "    #for agent, reward_history in rewards.items():\n",
    "    #    plt.plot(reward_history, label=f\"{agent} ({config['agent_types'].get(agent, 'iql')})\")\n",
    "    plot_training_results(rewards, agents, game, save_local=True)\n",
    "\n",
    "    save_notebook_to_html()\n",
    "    \n",
    "    wandb.finish()\n",
    "    #plt.title(f\"Config: {config['game']}\")\n",
    "    #plt.legend()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f305130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(game, agents, eval_config, wandb_run=None):\n",
    "    \"\"\"\n",
    "    Ejecuta evaluación sin aprendizaje con logging a W&B\n",
    "    \n",
    "    Args:\n",
    "        game: Entorno del juego\n",
    "        agents: Diccionario de agentes (deben tener learn=False)\n",
    "        eval_config: Configuración {'episodes': int}\n",
    "        wandb_run: Objeto de ejecución de W&B (opcional)\n",
    "    \"\"\"\n",
    "    # Desactivar aprendizaje en todos los agentes\n",
    "    for agent in agents.values():\n",
    "        if hasattr(agent, 'learn'):\n",
    "            agent.learn = False\n",
    "    \n",
    "    # Estadísticas de evaluación\n",
    "    eval_stats = {\n",
    "        'episode_rewards': {agent_id: [] for agent_id in agents},\n",
    "        'actions_taken': {agent_id: [] for agent_id in agents}\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== Evaluación Post-Entrenamiento ===\")\n",
    "    print(f\"Ejecutando {eval_config['episodes']} episodios de evaluación...\")\n",
    "    \n",
    "    for episode in tqdm(range(eval_config['episodes']), desc=\"Evaluating\"):\n",
    "        # Ejecutar episodio\n",
    "        rewards, _ = play_episode(game, agents)\n",
    "        \n",
    "        # Registrar acciones y recompensas\n",
    "        for agent_id in agents:\n",
    "            eval_stats['episode_rewards'][agent_id].append(rewards[agent_id])\n",
    "            eval_stats['actions_taken'][agent_id].append(agents[agent_id].last_action)\n",
    "        \n",
    "        # Loggear en W&B si está disponible\n",
    "        if wandb_run:\n",
    "            wandb.log({\n",
    "                **{f\"eval_episode/{agent_id}_reward\": rewards[agent_id] for agent_id in agents},\n",
    "                \"eval_episode\": episode\n",
    "            })\n",
    "    \n",
    "    # Calcular métricas agregadas\n",
    "    for agent_id in agents:\n",
    "        avg_reward = np.mean(eval_stats['episode_rewards'][agent_id])\n",
    "        eval_stats['avg_rewards'] = eval_stats.get('avg_rewards', {})\n",
    "        eval_stats['avg_rewards'][agent_id] = avg_reward\n",
    "        \n",
    "        if wandb_run:\n",
    "            # Loggear métricas finales de evaluación\n",
    "            wandb.log({\n",
    "                f\"eval_final/{agent_id}_avg_reward\": avg_reward,\n",
    "                f\"eval_final/{agent_id}_action_distribution\": wandb.Histogram(\n",
    "                    np.array(eval_stats['actions_taken'][agent_id])\n",
    "                )\n",
    "            })\n",
    "    \n",
    "    return eval_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fcb7ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de configuraciones comparativas\n",
    "configs = [\n",
    "    {\n",
    "        'game': \"Foraging-8x8-2p-1f-v3\",\n",
    "        'iql': IQLAgentConfig(),\n",
    "        'jal': JALAgentConfig(),\n",
    "        'train_config': {'episodes': 100, 'iterations': 30},\n",
    "        'eval_config': {'episodes': 50},\n",
    "        'agent_types': {'agent_0': 'jal', 'agent_1': 'jal'}\n",
    "    },\n",
    "    {\n",
    "        'game': game_config[3],  # Foraging-8x8-3p-1f-coop-v3\n",
    "        'iql': IQLAgentConfig(alpha=0.2),\n",
    "        'jal': JALAgentConfig(),\n",
    "        'train_config': {'episodes': 200, 'iterations': 30},\n",
    "        'eval_config': {'episodes': 50},\n",
    "        'agent_types': {'agent_0': 'iql', 'agent_1': 'iql', 'agent_2': 'iql'}\n",
    "    }\n",
    "]\n",
    "\n",
    "#for config in configs:\n",
    "#   run_configuration(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af0c2501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(config):\n",
    "    \"\"\"\n",
    "    Entrenamiento completo + evaluación con logging integrado\n",
    "    \"\"\"\n",
    "    # Configurar W&B\n",
    "    wandb_run = setup_wandb(config, project_name=\"foraging_experiments\", group='iql vs iql')\n",
    "    train_config = config['train_config']\n",
    "    eval_config = config['eval_config']\n",
    "    game = Foraging(config=config['game'], seed=config.get('seed', 1))\n",
    "    agents = {}\n",
    "    for agent_id in game.agents:\n",
    "        agent_type = config['agent_types'].get(agent_id, 'iql')\n",
    "        if agent_type == 'jal':\n",
    "            agents[agent_id] = JALAgent(game, agent_id, config['jal'])\n",
    "        else:\n",
    "            agents[agent_id] = IQLAgent(game, agent_id, config['iql'])\n",
    "    try:\n",
    "        # Fase de Entrenamiento\n",
    "        train_stats = train(game, agents, train_config)\n",
    "        \n",
    "        # Fase de Evaluación (sin aprendizaje)\n",
    "        eval_stats = run_evaluation(game, agents, eval_config, wandb_run)\n",
    "        \n",
    "        # Visualización de resultados\n",
    "        plot_combined_results(train_stats, eval_stats, agents)\n",
    "        \n",
    "        return {\n",
    "            'train_stats': train_stats,\n",
    "            'eval_stats': eval_stats\n",
    "        }\n",
    "        \n",
    "    finally:\n",
    "        if wandb_run:\n",
    "            wandb_run.finish()\n",
    "        save_notebook_to_html()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44a50448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/martinrizzo/Documents/Master IA/Sistemas Multiagente/leandro_repo/Multiagentes/Simultaneous_Games/wandb/run-20250520_234013-lvmyaqth</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rizzo33-universidad-ort-uruguay/foraging_experiments/runs/lvmyaqth' target=\"_blank\">Foraging_5x5_2pp_1ff_JAL0-IQL2_0520-2340</a></strong> to <a href='https://wandb.ai/rizzo33-universidad-ort-uruguay/foraging_experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rizzo33-universidad-ort-uruguay/foraging_experiments' target=\"_blank\">https://wandb.ai/rizzo33-universidad-ort-uruguay/foraging_experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rizzo33-universidad-ort-uruguay/foraging_experiments/runs/lvmyaqth' target=\"_blank\">https://wandb.ai/rizzo33-universidad-ort-uruguay/foraging_experiments/runs/lvmyaqth</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/pettingzoo_games/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting training for 50 iterations (5000 total episodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/5000 [00:00<?, ?episode/s]/usr/local/Caskroom/miniconda/base/envs/pettingzoo_games/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:275: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'list'>\u001b[0m\n",
      "  logger.warn(\n",
      "Training Progress: 100%|██████████| 5000/5000 [00:27<00:00, 184.87episode/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluación Post-Entrenamiento ===\n",
      "Ejecutando 20 episodios de evaluación...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>▁▁▂▂▂▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇█████████</td></tr><tr><td>episode_time</td><td>█▄▃▅▂▅▂▃▂▆▃▃▁▂▂▁▁▂▁▁▁▄▄▂▃▂▁▃▂▂▂▂▁▂▁▂▁▂▁▁</td></tr><tr><td>exploration/agent_0</td><td>██▆▆▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>exploration/agent_1</td><td>█▇▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>exploration_rate/agent_0</td><td>█▇▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>exploration_rate/agent_1</td><td>█▇▆▆▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>global_step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>iteration</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>iteration_avg_reward/agent_0</td><td>▁▃▃▄▃▄▃▅▄▄▆▄▄▆▅▆▆▄▆▆▇▄▆▇▅▇▆▇▇▇▆█▆▆█▇█▆▇▇</td></tr><tr><td>iteration_avg_reward/agent_1</td><td>▄▃▅▅▇▃▆█▆▇▅▄▇▄▇▆█▅▄▇▄▃▇▅▂▄▂▃▃▃▄▄▁▄▄▂▁▃▂▂</td></tr><tr><td>iteration_time</td><td>█▇▇▆▆▇▇▅▅▄▄▆▄▂▂▂▂▃▂▂▂▂▂▂▂▂▂▁▁▁▂▂▁▂▂▁▂▃▂▁</td></tr><tr><td>live_rewards/agent_0</td><td>▁█▁████▁████▁▁██▁▁█▁▁████▁█▁██▁▁██▅█▅██▅</td></tr><tr><td>live_rewards/agent_1</td><td>▁██▁▁▁██▁▁█▁▁▁███▁▁█▁██▁▁▁▁▁▁▁▅▁█▅▅██▁▁▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_episode_time</td><td>0.00506</td></tr><tr><td>avg_iteration_time</td><td>0.54041</td></tr><tr><td>episode</td><td>5000</td></tr><tr><td>episode_time</td><td>0.00242</td></tr><tr><td>episodes_per_second</td><td>184.75229</td></tr><tr><td>exploration/agent_0</td><td>0.10856</td></tr><tr><td>exploration/agent_1</td><td>0.10856</td></tr><tr><td>exploration_rate/agent_0</td><td>0.10979</td></tr><tr><td>exploration_rate/agent_1</td><td>0.10979</td></tr><tr><td>final_avg_reward/agent_0</td><td>0.695</td></tr><tr><td>final_avg_reward/agent_1</td><td>0.305</td></tr><tr><td>global_step</td><td>5000</td></tr><tr><td>iteration</td><td>50</td></tr><tr><td>iteration_avg_reward/agent_0</td><td>0.695</td></tr><tr><td>iteration_avg_reward/agent_1</td><td>0.305</td></tr><tr><td>iteration_time</td><td>0.27638</td></tr><tr><td>live_rewards/agent_0</td><td>0</td></tr><tr><td>live_rewards/agent_1</td><td>1</td></tr><tr><td>total_training_time</td><td>27.06326</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Foraging_5x5_2pp_1ff_JAL0-IQL2_0520-2340</strong> at: <a href='https://wandb.ai/rizzo33-universidad-ort-uruguay/foraging_experiments/runs/lvmyaqth' target=\"_blank\">https://wandb.ai/rizzo33-universidad-ort-uruguay/foraging_experiments/runs/lvmyaqth</a><br> View project at: <a href='https://wandb.ai/rizzo33-universidad-ort-uruguay/foraging_experiments' target=\"_blank\">https://wandb.ai/rizzo33-universidad-ort-uruguay/foraging_experiments</a><br>Synced 5 W&B file(s), 3 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_234013-lvmyaqth/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43miql_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mtrain_and_evaluate\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m     19\u001b[39m train_stats = train(game, agents, train_config)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Fase de Evaluación (sin aprendizaje)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m eval_stats = \u001b[43mrun_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb_run\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Visualización de resultados\u001b[39;00m\n\u001b[32m     25\u001b[39m plot_combined_results(train_stats, eval_stats, agents)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mrun_evaluation\u001b[39m\u001b[34m(game, agents, eval_config, wandb_run)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Registrar acciones y recompensas\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent_id \u001b[38;5;129;01min\u001b[39;00m agents:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     eval_stats[\u001b[33m'\u001b[39m\u001b[33mepisode_rewards\u001b[39m\u001b[33m'\u001b[39m][agent_id].append(\u001b[43mrewards\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent_id\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m     32\u001b[39m     eval_stats[\u001b[33m'\u001b[39m\u001b[33mactions_taken\u001b[39m\u001b[33m'\u001b[39m][agent_id].append(agents[agent_id].last_action)\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Loggear en W&B si está disponible\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(iql_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pettingzoo_games",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
