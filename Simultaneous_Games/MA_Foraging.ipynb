{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89a56b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lbforaging\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "os.environ['WANDB_SILENT'] = \"false\"\n",
    "os.environ['WANDB_START_METHOD'] = \"thread\"\n",
    "os.environ['WANDB_MODE'] = \"offline\"\n",
    "\n",
    "from games.foraging import Foraging\n",
    "from agents.iql_agent import IQLAgent, IQLAgentConfig\n",
    "from agents.jal_am_agent import JALAgent, JALAgentConfig\n",
    "from utils import plot_training_results\n",
    "import wandb\n",
    "from nbconvert import HTMLExporter\n",
    "import nbformat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d195856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_login():\n",
    "    wandb.login()\n",
    "\n",
    "def setup_wandb(config, project_name=\"foraging_rl\", group=\"jal_vs_iql\"):\n",
    "    \"\"\"Configura Weights & Biases para tracking de experimentos\"\"\"\n",
    "    if wandb.run is not None:\n",
    "        wandb.finish()\n",
    "\n",
    "    wandb.init(\n",
    "        project=project_name,\n",
    "        entity=\"rizzo33-universidad-ort-uruguay\",  # Cambia esto por tu entidad de W&B\n",
    "        group=group,\n",
    "        config=config,\n",
    "        save_code=False,\n",
    "        reinit=True,\n",
    "        settings=wandb.Settings(\n",
    "            disable_job_creation=True,  # Evitar problemas con notebooks\n",
    "            silent=False                # Reducir mensajes de consola\n",
    "        )\n",
    "    )\n",
    "    return wandb\n",
    "\n",
    "def save_notebook_to_html(notebook_path=\"MA_Foraging.ipynb\"):\n",
    "    \"\"\"Guarda el notebook como HTML\"\"\"\n",
    "    exporter = HTMLExporter()\n",
    "    notebook = nbformat.read(notebook_path, as_version=4)\n",
    "    html, _ = exporter.from_notebook_node(notebook)\n",
    "    \n",
    "    os.makedirs(\"notebook_exports\", exist_ok=True)\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    output_path = f\"notebook_exports/experiment_{timestamp}.html\"\n",
    "    \n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(html)\n",
    "    \n",
    "    if wandb.run:\n",
    "        wandb.save(output_path)\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41f01c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"8c15e4a23489e53756d79a4ac60076b117e45813\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bddb364",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_config = [\n",
    "        \"Foraging-5x5-2p-1f-v3\",\n",
    "        \"Foraging-8x8-2p-1f-v3\",\n",
    "        \"Foraging-8x8-3p-1f-v3\",\n",
    "        \"Foraging-8x8-3p-1f-coop-v3\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78c9acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n mixta\n",
    "mixed_config = {\n",
    "    'game': game_config[0],  # Foraging-5x5-2p-1f-v3\n",
    "    'iql': IQLAgentConfig(alpha=0.1, min_epsilon=0.01, max_t=10000, seed=1),\n",
    "    'jal': JALAgentConfig(alpha=0.1, min_epsilon=0.05, epsilon_decay=0.99999, max_t=10000, seed=1),\n",
    "    'train_config': {\n",
    "        'episodes': 100,\n",
    "        'iterations': 50,\n",
    "    },\n",
    "    'agent_types': {\n",
    "        'agent_0': 'jal',  # Primer agente usa JAL\n",
    "        'agent_1': 'jal'   # Segundo agente usa IQL\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "coop_config = {\n",
    "    'game': game_config[3],  # Foraging-8x8-3p-1f-coop-v3\n",
    "    'iql': IQLAgentConfig(alpha=0.1, min_epsilon=0.01, max_t=10000, seed=1),\n",
    "    'jal': JALAgentConfig(alpha=0.1, min_epsilon=0.05, epsilon_decay=0.99999, max_t=10000, seed=1),\n",
    "    'train_config': {\n",
    "        'episodes': 100,\n",
    "        'iterations': 50,\n",
    "    },\n",
    "    'agent_types': {\n",
    "        'agent_0': 'jal',  # Primer agente usa JAL\n",
    "        'agent_1': 'jal',   # Segundo agente usa JAL\n",
    "        'agent_2': 'jal'   # Tercer agente usa JAL\n",
    "    }\n",
    "}\n",
    "#wandb = setup_wandb(mixed_config, project_name=\"mixed_foraging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b463e8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/pettingzoo_games/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "# Crear el juego\n",
    "game = Foraging(config=mixed_config['game'], seed=1)\n",
    "\n",
    "# Crear agentes mixtos\n",
    "agents = {}\n",
    "for agent_id in game.agents:\n",
    "    if mixed_config['agent_types'].get(agent_id, 'iql') == 'jal':\n",
    "        agents[agent_id] = JALAgent(game, agent_id, coop_config['jal'])\n",
    "    else:\n",
    "        agents[agent_id] = IQLAgent(game, agent_id, coop_config['iql'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85dadbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: agent_0\n",
      "Observation: [3. 1. 2. 2. 3. 2. 0. 0. 2.]\n",
      "Agent: agent_1\n",
      "Observation: [3. 1. 2. 0. 0. 2. 2. 3. 2.]\n"
     ]
    }
   ],
   "source": [
    "game.reset()\n",
    "for agent in game.agents:\n",
    "    print(f\"Agent: {agent}\")\n",
    "    print(f\"Observation: {game.observe(agent)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbc0878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(game, agents, verbose=False, render=False):\n",
    "\n",
    "    # Initialize the game\n",
    "    game.reset()\n",
    "    step_count = 0\n",
    "\n",
    "    # Initialize each agent\n",
    "    for agent in game.agents:\n",
    "        agents[agent].reset()\n",
    "\n",
    "    # Print initial observations if verbose is enabled\n",
    "    if verbose:\n",
    "        print(f\"Step: {step_count}\")\n",
    "        for agent in game.agents:\n",
    "            print(f\"Agent {agent} observe: {game.observe(agent)}\")\n",
    "\n",
    "    # Initialize rewards for each agent\n",
    "    cum_rewards = dict(map(lambda agent: (agent, 0.0), game.agents))\n",
    "\n",
    "    # render the game if required\n",
    "    if render:\n",
    "        game.render()\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    while not game.done():\n",
    "\n",
    "        step_count += 1\n",
    "        \n",
    "        # Get actions from each agent\n",
    "        actions = {}\n",
    "        for agent in game.agents:\n",
    "            actions[agent] = agents[agent].action()\n",
    "             \n",
    "        # Perform the actions in the game\n",
    "        game.step(actions)\n",
    "\n",
    "        # Update the cum_rewards for each agent\n",
    "        for agent in game.agents:\n",
    "            cum_rewards[agent] += game.reward(agent)\n",
    "\n",
    "        # Print actions, rewards and next state if verbose is enabled\n",
    "        if verbose:\n",
    "            print(f\"Step: {step_count}\")\n",
    "            for agent in game.agents:\n",
    "                    print(f\"Agent {agent} action: {actions[agent]} - {game.action_set[actions[agent]]}\")\n",
    "                    print(f\"Agent {agent} reward: {game.reward(agent)}\")\n",
    "                    print(f\"Agent {agent} observe: {game.observe(agent)}\")\n",
    "            \n",
    "        if render:\n",
    "            game.render()\n",
    "            time.sleep(0.5)\n",
    "    \n",
    "        for agent in game.agents:\n",
    "            # Update the agent with the last observation\n",
    "            agents[agent].update()\n",
    "    \n",
    "    return cum_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfa8ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(game, agents, episodes=1, verbose=False, render=False, log_wandb=False):\n",
    "    sum_rewards = dict(map(lambda agent: (agent, 0.0), game.agents))\n",
    "    episodes_length = []\n",
    "    for _ in range(episodes):\n",
    "        episode_start = time.time()\n",
    "        cum_rewards = play_episode(game, agents, verbose=verbose, render=render)\n",
    "        episode_time = time.time() - episode_start\n",
    "        for agent in game.agents:\n",
    "            sum_rewards[agent] += cum_rewards[agent]\n",
    "        episodes_length.append(episode_time)\n",
    "\n",
    "        if log_wandb and wandb.run:\n",
    "            log_data = {f\"episode_reward/{agent}\": reward for agent, reward in cum_rewards.items()}\n",
    "            log_data[\"episode_length\"] = episode_time\n",
    "            wandb.log(log_data)\n",
    "        \n",
    "    if verbose:\n",
    "        print(f\"Average rewards over {episodes} episodes:\")\n",
    "        for agent in game.agents:\n",
    "            print(f\"Agent {agent}: {sum_rewards[agent] / episodes}\")\n",
    "    avg_rewards = {agent: sum_rewards[agent] / episodes for agent in game.agents}        \n",
    "    if log_wandb and wandb.run:\n",
    "        wandb.log({f\"avg_reward/{agent}\": avg_r for agent, avg_r in avg_rewards.items()})\n",
    "        wandb.log({\"avg_episode_time\": np.mean(episodes_length)})\n",
    "      \n",
    "    return sum_rewards     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3727d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(game, agents, train_config, progress=False, verbose=False, render=False):\n",
    "    iterations = train_config[\"iterations\"]\n",
    "    episodes = train_config[\"episodes\"]\n",
    "    total_episodes = iterations * episodes\n",
    "    \n",
    "    # Estad√≠sticas de entrenamiento\n",
    "    stats = {\n",
    "        'average_rewards': {agent: [] for agent in game.agents},\n",
    "        'episode_rewards': {agent: [] for agent in game.agents},\n",
    "        'exploration_rates': {agent: [] for agent in game.agents},\n",
    "        'time_per_episode': [],\n",
    "        'total_time': 0\n",
    "    }\n",
    "    \n",
    "    print(f\"üöÄ Starting training for {iterations} iterations ({total_episodes} total episodes)\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Configurar barra de progreso principal\n",
    "    pbar = tqdm(total=total_episodes, desc=\"Training Progress\", unit=\"episode\", disable=not progress)\n",
    "    \n",
    "    for i in range(1, iterations + 1):\n",
    "        iteration_rewards = {agent: 0.0 for agent in game.agents}\n",
    "        iteration_exploration = {agent: 0.0 for agent in game.agents}\n",
    "        \n",
    "        for e in range(1, episodes + 1):\n",
    "            episode_start = time.time()\n",
    "            episode_num = (i - 1) * episodes + e\n",
    "            \n",
    "            # Ejecutar episodio\n",
    "            cum_rewards = play_episode(game, agents, verbose=verbose, render=render)\n",
    "            \n",
    "            # Calcular tiempo del episodio\n",
    "            episode_time = time.time() - episode_start\n",
    "            stats['time_per_episode'].append(episode_time)\n",
    "            \n",
    "            # Actualizar estad√≠sticas\n",
    "            for agent in game.agents:\n",
    "                stats['episode_rewards'][agent].append(cum_rewards[agent])\n",
    "                iteration_rewards[agent] += cum_rewards[agent]\n",
    "                \n",
    "                # Registrar exploraci√≥n (para agentes que la tengan)\n",
    "                if hasattr(agents[agent], 'config') and hasattr(agents[agent].config, 'initial_epsilon'):\n",
    "                    iteration_exploration[agent] += agents[agent].config.initial_epsilon\n",
    "            \n",
    "            # Loggear en W&B\n",
    "            if wandb.run:\n",
    "                wandb.log({\n",
    "                    **{f\"live_rewards/{agent}\": cum_rewards[agent] for agent in game.agents},\n",
    "                    \"episode\": episode_num,\n",
    "                    \"episode_time\": episode_time\n",
    "                })\n",
    "            # Actualizar barra de progreso\n",
    "            #pbar.set_postfix({\n",
    "            #    'Iter': f\"{i}/{iterations}\",\n",
    "            #    'Last Rewards': {a: f\"{r:.1f}\" for a, r in cum_rewards.items()},\n",
    "            #    'Time/ep': f\"{episode_time:.2f}s\"\n",
    "            #})\n",
    "            pbar.update(1)\n",
    "        \n",
    "        # Guardar promedios por iteraci√≥n\n",
    "        for agent in game.agents:\n",
    "            avg_reward = iteration_rewards[agent] / episodes\n",
    "            stats['average_rewards'][agent].append(avg_reward)\n",
    "            stats['exploration_rates'][agent].append(iteration_exploration[agent] / episodes)\n",
    "\n",
    "            # Loggear en W&B\n",
    "            if wandb.run:\n",
    "                wandb.log({\n",
    "                    f\"iteration_avg_reward/{agent}\": avg_reward,\n",
    "                    f\"exploration_rate/{agent}\": stats['exploration_rates'][agent][-1],\n",
    "                    \"iteration\": i\n",
    "                })\n",
    "\n",
    "    # Finalizar medici√≥n de tiempo\n",
    "    stats['total_time'] = time.time() - start_time\n",
    "    pbar.close()\n",
    "\n",
    "    # Guardar resultados finales en W&B\n",
    "    if wandb.run:\n",
    "        wandb.log({\n",
    "            \"total_training_time\": stats['total_time'],\n",
    "            \"avg_episode_time\": np.mean(stats['time_per_episode'])\n",
    "        })\n",
    "        \n",
    "        # Guardar figuras en W&B\n",
    "        figures = plot_training_results(stats, agents, game, save_local=False)\n",
    "        for name, fig in figures.items():\n",
    "            wandb.log({f\"charts/{name}\": wandb.Image(fig)})\n",
    "            plt.close(fig)\n",
    "    \n",
    "    # Mostrar resumen final\n",
    "    # if progress:\n",
    "    #     print(f\"\\n‚úÖ Training completed in {stats['total_time']:.2f} seconds\")\n",
    "    #     print(f\"‚è±Ô∏è Average time per episode: {np.mean(stats['time_per_episode']):.2f}s\")\n",
    "        \n",
    "    #     print(\"\\nüèÜ Final Performance:\")\n",
    "    #     for agent in game.agents:\n",
    "    #         agent_type = type(agents[agent]).__name__\n",
    "    #         final_avg = np.mean(stats['average_rewards'][agent][-5:])  # √öltimas 5 iteraciones\n",
    "    #         best_episode = np.max(stats['episode_rewards'][agent])\n",
    "            \n",
    "    #         print(f\"\\nAgent {agent} ({agent_type}):\")\n",
    "    #         print(f\"  ‚Ä¢ Final Avg Reward: {final_avg:.2f}\")\n",
    "    #         print(f\"  ‚Ä¢ Best Episode Reward: {best_episode:.2f}\")\n",
    "    #         print(f\"  ‚Ä¢ Final Exploration: {stats['exploration_rates'][agent][-1]:.3f}\")\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedb0060",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = agents[game.agents[0]].q_table\n",
    "print(f\"Q-table for agent {game.agents[0]}:\")\n",
    "print(f\"Estados: {len(q_table.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ae4961c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in game.agents:\n",
    "    agents[agent].learn = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2a719562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 2. 3. 2. 0. 0. 2.]\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 0. 2. 2. 3. 2.]\n",
      "Step: 1\n",
      "Agent agent_0 action: 2 - SOUTH\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 3. 2. 1. 0. 2.]\n",
      "Agent agent_1 action: 2 - SOUTH\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 1. 0. 2. 3. 3. 2.]\n",
      "Step: 2\n",
      "Agent agent_0 action: 5 - LOAD\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 3. 2. 1. 1. 2.]\n",
      "Agent agent_1 action: 4 - EAST\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 1. 1. 2. 3. 3. 2.]\n",
      "Step: 3\n",
      "Agent agent_0 action: 3 - WEST\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 2. 2. 0. 1. 2.]\n",
      "Agent agent_1 action: 1 - NORTH\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 1. 2. 3. 2. 2.]\n",
      "Step: 4\n",
      "Agent agent_0 action: 5 - LOAD\n",
      "Agent agent_0 reward: 1.0\n",
      "Agent agent_0 observe: [-1. -1.  0.  3.  2.  2.  0.  1.  2.]\n",
      "Agent agent_1 action: 5 - LOAD\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [-1. -1.  0.  0.  1.  2.  3.  2.  2.]\n",
      "Step: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 2. 3. 2. 0. 0. 2.]\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 0. 2. 2. 3. 2.]\n",
      "Step: 1\n",
      "Agent agent_0 action: 2 - SOUTH\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 3. 2. 0. 1. 2.]\n",
      "Agent agent_1 action: 4 - EAST\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 1. 2. 3. 3. 2.]\n",
      "Step: 2\n",
      "Agent agent_0 action: 3 - WEST\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 2. 2. 0. 1. 2.]\n",
      "Agent agent_1 action: 1 - NORTH\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 1. 2. 3. 2. 2.]\n",
      "Step: 3\n",
      "Agent agent_0 action: 2 - SOUTH\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 4. 2. 2. 0. 1. 2.]\n",
      "Agent agent_1 action: 0 - NONE\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 1. 2. 4. 2. 2.]\n",
      "Step: 4\n",
      "Agent agent_0 action: 1 - NORTH\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 2. 2. 0. 1. 2.]\n",
      "Agent agent_1 action: 1 - NORTH\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 1. 2. 3. 2. 2.]\n",
      "Step: 5\n",
      "Agent agent_0 action: 5 - LOAD\n",
      "Agent agent_0 reward: 1.0\n",
      "Agent agent_0 observe: [-1. -1.  0.  3.  2.  2.  0.  0.  2.]\n",
      "Agent agent_1 action: 3 - WEST\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [-1. -1.  0.  0.  0.  2.  3.  2.  2.]\n",
      "Step: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 2. 3. 2. 0. 0. 2.]\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 0. 2. 2. 3. 2.]\n",
      "Step: 1\n",
      "Agent agent_0 action: 2 - SOUTH\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 3. 2. 0. 0. 2.]\n",
      "Agent agent_1 action: 1 - NORTH\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 0. 2. 3. 3. 2.]\n",
      "Step: 2\n",
      "Agent agent_0 action: 3 - WEST\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 2. 2. 0. 0. 2.]\n",
      "Agent agent_1 action: 5 - LOAD\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 0. 2. 3. 2. 2.]\n",
      "Step: 3\n",
      "Agent agent_0 action: 5 - LOAD\n",
      "Agent agent_0 reward: 1.0\n",
      "Agent agent_0 observe: [-1. -1.  0.  3.  2.  2.  0.  0.  2.]\n",
      "Agent agent_1 action: 1 - NORTH\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [-1. -1.  0.  0.  0.  2.  3.  2.  2.]\n",
      "Step: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 2. 3. 2. 0. 0. 2.]\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 0. 2. 2. 3. 2.]\n",
      "Step: 1\n",
      "Agent agent_0 action: 2 - SOUTH\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 3. 2. 0. 0. 2.]\n",
      "Agent agent_1 action: 0 - NONE\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 0. 2. 3. 3. 2.]\n",
      "Step: 2\n",
      "Agent agent_0 action: 3 - WEST\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 2. 2. 0. 0. 2.]\n",
      "Agent agent_1 action: 5 - LOAD\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 0. 2. 3. 2. 2.]\n",
      "Step: 3\n",
      "Agent agent_0 action: 5 - LOAD\n",
      "Agent agent_0 reward: 1.0\n",
      "Agent agent_0 observe: [-1. -1.  0.  3.  2.  2.  0.  1.  2.]\n",
      "Agent agent_1 action: 4 - EAST\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [-1. -1.  0.  0.  1.  2.  3.  2.  2.]\n",
      "Step: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 2. 3. 2. 0. 0. 2.]\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 0. 2. 2. 3. 2.]\n",
      "Step: 1\n",
      "Agent agent_0 action: 2 - SOUTH\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 3. 2. 0. 1. 2.]\n",
      "Agent agent_1 action: 4 - EAST\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 1. 2. 3. 3. 2.]\n",
      "Step: 2\n",
      "Agent agent_0 action: 1 - NORTH\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 2. 3. 2. 0. 1. 2.]\n",
      "Agent agent_1 action: 1 - NORTH\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 1. 2. 2. 3. 2.]\n",
      "Step: 3\n",
      "Agent agent_0 action: 1 - NORTH\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 1. 3. 2. 0. 1. 2.]\n",
      "Agent agent_1 action: 5 - LOAD\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 1. 2. 1. 3. 2.]\n",
      "Step: 4\n",
      "Agent agent_0 action: 0 - NONE\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 1. 3. 2. 1. 1. 2.]\n",
      "Agent agent_1 action: 2 - SOUTH\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 1. 1. 2. 1. 3. 2.]\n",
      "Step: 5\n",
      "Agent agent_0 action: 2 - SOUTH\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 2. 3. 2. 1. 0. 2.]\n",
      "Agent agent_1 action: 3 - WEST\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 1. 0. 2. 2. 3. 2.]\n",
      "Step: 6\n",
      "Agent agent_0 action: 2 - SOUTH\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 3. 2. 1. 0. 2.]\n",
      "Agent agent_1 action: 5 - LOAD\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 1. 0. 2. 3. 3. 2.]\n",
      "Step: 7\n",
      "Agent agent_0 action: 2 - SOUTH\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 4. 3. 2. 1. 0. 2.]\n",
      "Agent agent_1 action: 0 - NONE\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 1. 0. 2. 4. 3. 2.]\n",
      "Step: 8\n",
      "Agent agent_0 action: 1 - NORTH\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 3. 2. 2. 0. 2.]\n",
      "Agent agent_1 action: 2 - SOUTH\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 2. 0. 2. 3. 3. 2.]\n",
      "Step: 9\n",
      "Agent agent_0 action: 3 - WEST\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 2. 2. 2. 1. 2.]\n",
      "Agent agent_1 action: 4 - EAST\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 2. 1. 2. 3. 2. 2.]\n",
      "Step: 10\n",
      "Agent agent_0 action: 5 - LOAD\n",
      "Agent agent_0 reward: 1.0\n",
      "Agent agent_0 observe: [-1. -1.  0.  3.  2.  2.  2.  1.  2.]\n",
      "Agent agent_1 action: 2 - SOUTH\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [-1. -1.  0.  2.  1.  2.  3.  2.  2.]\n",
      "Step: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 2. 3. 2. 0. 0. 2.]\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 0. 2. 2. 3. 2.]\n",
      "Step: 1\n",
      "Agent agent_0 action: 2 - SOUTH\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 3. 2. 1. 0. 2.]\n",
      "Agent agent_1 action: 2 - SOUTH\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 1. 0. 2. 3. 3. 2.]\n",
      "Step: 2\n",
      "Agent agent_0 action: 3 - WEST\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 2. 2. 1. 0. 2.]\n",
      "Agent agent_1 action: 5 - LOAD\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 1. 0. 2. 3. 2. 2.]\n",
      "Step: 3\n",
      "Agent agent_0 action: 2 - SOUTH\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 4. 2. 2. 1. 0. 2.]\n",
      "Agent agent_1 action: 0 - NONE\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 1. 0. 2. 4. 2. 2.]\n",
      "Step: 4\n",
      "Agent agent_0 action: 1 - NORTH\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 2. 2. 1. 0. 2.]\n",
      "Agent agent_1 action: 3 - WEST\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 1. 0. 2. 3. 2. 2.]\n",
      "Step: 5\n",
      "Agent agent_0 action: 5 - LOAD\n",
      "Agent agent_0 reward: 1.0\n",
      "Agent agent_0 observe: [-1. -1.  0.  3.  2.  2.  1.  0.  2.]\n",
      "Agent agent_1 action: 3 - WEST\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [-1. -1.  0.  1.  0.  2.  3.  2.  2.]\n",
      "Step: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 2. 3. 2. 0. 0. 2.]\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 0. 2. 2. 3. 2.]\n",
      "Step: 1\n",
      "Agent agent_0 action: 0 - NONE\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 2. 3. 2. 0. 0. 2.]\n",
      "Agent agent_1 action: 1 - NORTH\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 0. 2. 2. 3. 2.]\n",
      "Step: 2\n",
      "Agent agent_0 action: 2 - SOUTH\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 3. 2. 0. 1. 2.]\n",
      "Agent agent_1 action: 4 - EAST\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 1. 2. 3. 3. 2.]\n",
      "Step: 3\n",
      "Agent agent_0 action: 3 - WEST\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 2. 2. 0. 2. 2.]\n",
      "Agent agent_1 action: 4 - EAST\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 2. 2. 3. 2. 2.]\n",
      "Step: 4\n",
      "Agent agent_0 action: 5 - LOAD\n",
      "Agent agent_0 reward: 1.0\n",
      "Agent agent_0 observe: [-1. -1.  0.  3.  2.  2.  1.  2.  2.]\n",
      "Agent agent_1 action: 2 - SOUTH\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [-1. -1.  0.  1.  2.  2.  3.  2.  2.]\n",
      "Step: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 2. 3. 2. 0. 0. 2.]\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 0. 2. 2. 3. 2.]\n",
      "Step: 1\n",
      "Agent agent_0 action: 2 - SOUTH\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 3. 2. 0. 0. 2.]\n",
      "Agent agent_1 action: 0 - NONE\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 0. 2. 3. 3. 2.]\n",
      "Step: 2\n",
      "Agent agent_0 action: 3 - WEST\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 2. 2. 0. 0. 2.]\n",
      "Agent agent_1 action: 3 - WEST\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 0. 2. 3. 2. 2.]\n",
      "Step: 3\n",
      "Agent agent_0 action: 5 - LOAD\n",
      "Agent agent_0 reward: 1.0\n",
      "Agent agent_0 observe: [-1. -1.  0.  3.  2.  2.  0.  1.  2.]\n",
      "Agent agent_1 action: 4 - EAST\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [-1. -1.  0.  0.  1.  2.  3.  2.  2.]\n",
      "Step: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 2. 3. 2. 0. 0. 2.]\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 0. 2. 2. 3. 2.]\n",
      "Step: 1\n",
      "Agent agent_0 action: 1 - NORTH\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 1. 3. 2. 1. 0. 2.]\n",
      "Agent agent_1 action: 2 - SOUTH\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 1. 0. 2. 1. 3. 2.]\n",
      "Step: 2\n",
      "Agent agent_0 action: 2 - SOUTH\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 2. 3. 2. 1. 0. 2.]\n",
      "Agent agent_1 action: 0 - NONE\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 1. 0. 2. 2. 3. 2.]\n",
      "Step: 3\n",
      "Agent agent_0 action: 5 - LOAD\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 2. 3. 2. 2. 0. 2.]\n",
      "Agent agent_1 action: 2 - SOUTH\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 2. 0. 2. 2. 3. 2.]\n",
      "Step: 4\n",
      "Agent agent_0 action: 2 - SOUTH\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 3. 2. 2. 0. 2.]\n",
      "Agent agent_1 action: 3 - WEST\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 2. 0. 2. 3. 3. 2.]\n",
      "Step: 5\n",
      "Agent agent_0 action: 3 - WEST\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 2. 2. 2. 0. 2.]\n",
      "Agent agent_1 action: 3 - WEST\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 2. 0. 2. 3. 2. 2.]\n",
      "Step: 6\n",
      "Agent agent_0 action: 5 - LOAD\n",
      "Agent agent_0 reward: 1.0\n",
      "Agent agent_0 observe: [-1. -1.  0.  3.  2.  2.  2.  1.  2.]\n",
      "Agent agent_1 action: 4 - EAST\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [-1. -1.  0.  2.  1.  2.  3.  2.  2.]\n",
      "Step: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 2. 3. 2. 0. 0. 2.]\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 0. 2. 2. 3. 2.]\n",
      "Step: 1\n",
      "Agent agent_0 action: 2 - SOUTH\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 3. 2. 0. 1. 2.]\n",
      "Agent agent_1 action: 4 - EAST\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 1. 2. 3. 3. 2.]\n",
      "Step: 2\n",
      "Agent agent_0 action: 3 - WEST\n",
      "Agent agent_0 reward: 0\n",
      "Agent agent_0 observe: [3. 1. 2. 3. 2. 2. 0. 1. 2.]\n",
      "Agent agent_1 action: 0 - NONE\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [3. 1. 2. 0. 1. 2. 3. 2. 2.]\n",
      "Step: 3\n",
      "Agent agent_0 action: 5 - LOAD\n",
      "Agent agent_0 reward: 1.0\n",
      "Agent agent_0 observe: [-1. -1.  0.  3.  2.  2.  0.  0.  2.]\n",
      "Agent agent_1 action: 3 - WEST\n",
      "Agent agent_1 reward: 0\n",
      "Agent agent_1 observe: [-1. -1.  0.  0.  0.  2.  3.  2.  2.]\n",
      "Average rewards over 10 episodes:\n",
      "Agent agent_0: 1.0\n",
      "Agent agent_1: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'agent_0': 10.0, 'agent_1': 0.0}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(game, agents, episodes=10, verbose=True, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3342562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_combined_results(train_stats, eval_stats, agents):\n",
    "    \"\"\"Visualizaci√≥n combinada de entrenamiento y evaluaci√≥n\"\"\"\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    # Gr√°fico 1: Progreso de entrenamiento\n",
    "    plt.subplot(1, 3, 1)\n",
    "    for agent_id in train_stats['average_rewards']:\n",
    "        plt.plot(train_stats['average_rewards'][agent_id], \n",
    "                label=f\"{agent_id} ({type(agents[agent_id]).__name__})\")\n",
    "    plt.title(\"Training Progress\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Average Reward\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gr√°fico 2: Recompensas de evaluaci√≥n\n",
    "    plt.subplot(1, 3, 2)\n",
    "    for agent_id in eval_stats['episode_rewards']:\n",
    "        rewards = eval_stats['episode_rewards'][agent_id]\n",
    "        plt.scatter(range(len(rewards)), rewards, alpha=0.5, \n",
    "                   label=f\"{agent_id} eval rewards\")\n",
    "    plt.title(\"Evaluation Episode Rewards\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gr√°fico 3: Distribuci√≥n de acciones en evaluaci√≥n\n",
    "    plt.subplot(1, 3, 3)\n",
    "    for agent_id in eval_stats['actions_taken']:\n",
    "        actions = eval_stats['actions_taken'][agent_id]\n",
    "        plt.hist(actions, alpha=0.5, bins=20, \n",
    "                label=f\"{agent_id} actions\")\n",
    "    plt.title(\"Evaluation Action Distribution\")\n",
    "    plt.xlabel(\"Action\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar en W&B\n",
    "    if wandb.run:\n",
    "        wandb.log({\"combined_results\": plt})\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e04231c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_configuration(config):\n",
    "    # Inicializar W&B\n",
    "    setup_wandb(config, project_name=\"foraging_experiments\", group='jal vs iql')\n",
    "    game = Foraging(config=config['game'], seed=config.get('seed', 1))\n",
    "    \n",
    "    agents = {}\n",
    "    for agent_id in game.agents:\n",
    "        agent_type = config['agent_types'].get(agent_id, 'iql')\n",
    "        if agent_type == 'jal':\n",
    "            agents[agent_id] = JALAgent(game, agent_id, config['jal'])\n",
    "        else:\n",
    "            agents[agent_id] = IQLAgent(game, agent_id, config['iql'])\n",
    "    \n",
    "    rewards = train(game, agents, config['train_config'], progress=True)\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    #for agent, reward_history in rewards.items():\n",
    "    #    plt.plot(reward_history, label=f\"{agent} ({config['agent_types'].get(agent, 'iql')})\")\n",
    "    plot_training_results(rewards, agents, game, save_local=True)\n",
    "\n",
    "    save_notebook_to_html()\n",
    "    \n",
    "    wandb.finish()\n",
    "    #plt.title(f\"Config: {config['game']}\")\n",
    "    #plt.legend()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f305130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(game, agents, eval_config, wandb_run=None):\n",
    "    \"\"\"\n",
    "    Ejecuta evaluaci√≥n sin aprendizaje con logging a W&B\n",
    "    \n",
    "    Args:\n",
    "        game: Entorno del juego\n",
    "        agents: Diccionario de agentes (deben tener learn=False)\n",
    "        eval_config: Configuraci√≥n {'episodes': int}\n",
    "        wandb_run: Objeto de ejecuci√≥n de W&B (opcional)\n",
    "    \"\"\"\n",
    "    # Desactivar aprendizaje en todos los agentes\n",
    "    for agent in agents.values():\n",
    "        if hasattr(agent, 'learn'):\n",
    "            agent.learn = False\n",
    "    \n",
    "    # Estad√≠sticas de evaluaci√≥n\n",
    "    eval_stats = {\n",
    "        'episode_rewards': {agent_id: [] for agent_id in agents},\n",
    "        'actions_taken': {agent_id: [] for agent_id in agents}\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== Evaluaci√≥n Post-Entrenamiento ===\")\n",
    "    print(f\"Ejecutando {eval_config['episodes']} episodios de evaluaci√≥n...\")\n",
    "    \n",
    "    for episode in tqdm(range(eval_config['episodes']), desc=\"Evaluating\"):\n",
    "        # Ejecutar episodio\n",
    "        rewards = play_episode(game, agents)\n",
    "        \n",
    "        # Registrar acciones y recompensas\n",
    "        for agent_id in agents:\n",
    "            eval_stats['episode_rewards'][agent_id].append(rewards[agent_id])\n",
    "            eval_stats['actions_taken'][agent_id].append(agents[agent_id].last_action)\n",
    "        \n",
    "        # Loggear en W&B si est√° disponible\n",
    "        if wandb_run:\n",
    "            wandb.log({\n",
    "                **{f\"eval_episode/{agent_id}_reward\": rewards[agent_id] for agent_id in agents},\n",
    "                \"eval_episode\": episode\n",
    "            })\n",
    "    \n",
    "    # Calcular m√©tricas agregadas\n",
    "    for agent_id in agents:\n",
    "        avg_reward = np.mean(eval_stats['episode_rewards'][agent_id])\n",
    "        eval_stats['avg_rewards'] = eval_stats.get('avg_rewards', {})\n",
    "        eval_stats['avg_rewards'][agent_id] = avg_reward\n",
    "        \n",
    "        if wandb_run:\n",
    "            # Loggear m√©tricas finales de evaluaci√≥n\n",
    "            wandb.log({\n",
    "                f\"eval_final/{agent_id}_avg_reward\": avg_reward,\n",
    "                f\"eval_final/{agent_id}_action_distribution\": wandb.Histogram(\n",
    "                    np.array(eval_stats['actions_taken'][agent_id])\n",
    "                )\n",
    "            })\n",
    "    \n",
    "    return eval_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fcb7ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/pettingzoo_games/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training for 30 iterations (3000 total episodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/3000 [00:00<?, ?episode/s]/usr/local/Caskroom/miniconda/base/envs/pettingzoo_games/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:275: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'list'>\u001b[0m\n",
      "  logger.warn(\n",
      "Training Progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:35<00:00, 84.71episode/s] \n",
      "/usr/local/Caskroom/miniconda/base/envs/pettingzoo_games/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training for 30 iterations (6000 total episodes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/6000 [00:00<?, ?episode/s]/usr/local/Caskroom/miniconda/base/envs/pettingzoo_games/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:275: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'list'>\u001b[0m\n",
      "  logger.warn(\n",
      "Training Progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [05:34<00:00, 17.94episode/s]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de configuraciones comparativas\n",
    "configs = [\n",
    "    {\n",
    "        'game': \"Foraging-8x8-2p-1f-v3\",\n",
    "        'iql': IQLAgentConfig(),\n",
    "        'jal': JALAgentConfig(),\n",
    "        'train_config': {'episodes': 100, 'iterations': 30},\n",
    "        'agent_types': {'agent_0': 'jal', 'agent_1': 'jal'}\n",
    "    },\n",
    "    {\n",
    "        'game': game_config[3],  # Foraging-8x8-3p-1f-coop-v3\n",
    "        'iql': IQLAgentConfig(alpha=0.2),\n",
    "        'jal': JALAgentConfig(),\n",
    "        'train_config': {'episodes': 200, 'iterations': 30},\n",
    "        'agent_types': {'agent_0': 'jal', 'agent_1': 'jal', 'agent_2': 'jal'}\n",
    "    }\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    run_configuration(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pettingzoo_games",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
